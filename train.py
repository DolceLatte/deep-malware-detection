import os

import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm


def train(
    model,
    train_loader,
    device,
    save_title,
    num_epochs=5,
    learning_rate=0.001,
    print_interval=50,
):
    log = []
    criterion = torch.nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    model.train()
    for epoch in range(num_epochs):
        for i, (inputs, labels) in tqdm(enumerate(train_loader)):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss = loss.item()
            log.append(loss)
            if (i + 1) % print_interval == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], "
                    f"Batch [{i + 1}/{len(train_loader)}], "
                    f"Loss: {loss:.4f}"
                )
    torch.save(
        {"model_state_dict": model.state_dict(), "loss": loss},
        os.path.join("weights", f"{save_title}.pt"),
    )
    plt.rcParams.update(
        {"text.usetex": True, "font.family": "serif", "font.serif": ["cm"],}
    )
    fig, ax = plt.subplots(1, 1, figsize=(8, 5))
    ax.scatter(range(len(log)), log, alpha=0.8)
    ax.grid(linestyle="--")
    ax.set_xlabel("Iterations")
    ax.set_ylabel("BCE Loss")
    num_batch = len(train_loader)
    epoch_ticks = num_batch * np.arange(1, num_epochs + 1)
    ax.set_xticks(epoch_ticks)
    ax.set_xticklabels([f"Epoch {i + 1}" for i in range(len(epoch_ticks))])
    fig.savefig(os.path.join("figs", f"{save_title}.png"), dpi=300)
