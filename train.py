import os

import torch
from torch import optim
from tqdm import tqdm

from utils import plot_train_history, predict


def train(
    model,
    train_loader,
    val_loader,
    device,
    save_title,
    num_epochs=10,
    patience=2,
    learning_rate=0.001,
    print_interval=50,
):
    history = []
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, "max", patience=patience
    )
    model.train()
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(tqdm(train_loader, leave=False)):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss = loss.item()
            history.append(loss)
            if (i + 1) % print_interval == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], "
                    f"Batch [{i + 1}/{len(train_loader)}], "
                    f"Loss: {loss:.4f}"
                )
        y_true, y_pred = predict(model, val_loader, device, to_numpy=False)
        accuracy = 100 * (y_true == y_pred).sum().item() / y_true.size(0)
        scheduler.step(accuracy)
    torch.save(model.state_dict(), os.path.join("weights", f"{save_title}.pt"))
    plot_train_history(history, num_epochs, len(train_loader), save_title)
