import os

import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm
from torch.optim


def train(
    model,
    train_loader,
    device,
    save_title,
    num_epochs=5,
    learning_rate=0.0001,
    print_interval=50,
):
    log = []
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    model.train()
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(tqdm(train_loader, leave=False)):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss = loss.item()
            log.append(loss)
            if (i + 1) % print_interval == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], "
                    f"Batch [{i + 1}/{len(train_loader)}], "
                    f"Loss: {loss:.4f}"
                )
    torch.save(model.state_dict(), os.path.join("weights", f"{save_title}.pt"))
    fig, ax = plt.subplots()
    ax.plot(range(len(log)), log, color="blue")
    ax.grid(linestyle="--")
    ax.set_xlabel("Iterations")
    ax.set_ylabel("BCE Loss")
    num_batch = len(train_loader)
    epoch_ticks = num_batch * np.arange(1, num_epochs + 1)
    ax.set_xticks(epoch_ticks)
    ax.set_xticklabels([f"Epoch {i + 1}" for i in range(len(epoch_ticks))])
    fig.savefig(os.path.join("figs", f"{save_title}.png"), dpi=300)
