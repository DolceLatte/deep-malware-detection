import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn


def train(
    model,
    train_loader,
    device,
    fig_title,
    num_epochs=5,
    learning_rate=0.0001,
    print_interval=50,
    save_path="weights",
):
    log = []
    criterion = nn.BCELoss()
    optimizer = nn.optim.Adam(model.parameters(), lr=learning_rate)
    model.train()
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(train_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss = loss.item()
            log.append(loss)
            if (i + 1) % print_interval == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], "
                    f"Batch [{i + 1}/{len(train_loader)}], "
                    f"Loss: {loss:.4f}"
                )
    torch.save({"model_state_dict": model.state_dict(), "loss": loss}, path)
    fig, ax = plt.subplots()
    ax.scatter(range(len(log)), log, alpha=0.8)
    ax.set_xlabel("Iterations")
    ax.set_ylabel("BCE Loss")
    num_batch = len(train_loader)
    epoch_ticks = num_batch * np.arange(1, num_epochs + 1)
    ax.set_xticks(epoch_ticks)
    ax.set_xticklabels([f"Epoch {i + 1}" for i in range(len(epoch_ticks))])
    fig.savefig(fig_title, dpi=300)
