import matplotlib.pyplot as plt
import numpy as np
from torch import nn


def train(
    model, train_loader, num_epochs, learning_rate, device, print_interval, fig_title
):
    log = []
    criterion = nn.BCELoss()
    optimizer = nn.optim.Adam(model.parameters(), lr=learning_rate)
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(train_loader):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss = loss.item()
            log.append(loss)
            if (i + 1) % print_interval == 0:
                print(
                    f"Epoch [{epoch + 1}/{num_epochs}], "
                    f"Batch [{i + 1}/{len(train_loader)}], "
                    f"Loss: {loss:.4f}"
                )
    fig, ax = plt.subplots()
    ax.plot(range(len(log)), log)
    ax.set_xlabel("Iterations")
    ax.set_ylabel("BCE Loss")
    num_batch = len(train_loader)
    epoch_ticks = num_batch * np.arange(1, num_epochs + 1)
    ax.set_xticks(epoch_ticks)
    ax.set_xticklabels([f"Epoch {i + 1}" for i in range(len(epoch_ticks))])
    fig.savefig(fig_title, dpi=300)
