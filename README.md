# Neural Network Malware Binary Classification

PyTorch implementation of [1] [Malware Detection by Eating a Whole EXE](https://arxiv.org/abs/1710.09435), [2] [Learning the PE Header, Malware Detection with Minimal Domain Knowledge](https://arxiv.org/abs/1709.01471), and other derived custom models, including transformers based on [3] [DistilBERT](https://arxiv.org/abs/1910.01108) and [4] [LongFormer](https://arxiv.org/abs/2004.05150) for malware detection.

## Quickstart

Clone this repository via

```
git clone https://github.com/jaketae/pytorch-malware-detection.git
cd pytorch-malware-detection
```

Then, a Python virtual environment:

```
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

If you have [pipenv](https://pypi.org/project/pipenv/), you can also type

```
pipenv install -r requirements.txt
```

Train the model in Jupyter notebook titled `run.ipynb`, or start training through the terminal via

```
python train.py
```

Optional flags are documented below.

## Implementation Notes

1. While the [2] used LSTMs for the sequential model, we tested both GRU and LSTMs and found that the former was easier to train.
2. We combined models presented in papers [1] and [2] to derive a custom model that uses concatenated feature vector produced by the entry point 1D-CNN layer as well as the RNN units that follow. We denote these custom models with a "Res" prefix in the table below.
3. We also further develop the attention-based model in [2] with this residual approach.
4. While the [1] used the entire binary of PE files, our approach more closely resembles that of [2]. Due to computational constraints, we decided to only use PE file headers up to their 4096th bytes, thus creating a 4096 dimensional sequential feature vector for every file.
5. BERT-based transformers are not designed to process long documents to due their `O(n^2)` runtime attention mechanism. To somewhat remedy this limitation, we use DistilBERT, which is a lighter model with fewer layers. We also use LongFormers, which use a sliding window and global-local attention mechanisms to reduce the runtime to be linear, or `O(n)`. Due to computational resource constratins, we have not tested these configurations yet.

## Results

| Architecture   | # Parameters | Acc | F1  | AUC |
| -------------- | ------------ | --- | --- | --- |
| MalConvBase    |              |     |     |     |
| MalConvPlus    |              |     |     |     |
| GRU-CNN        |              |     |     |     |
| LSTM-CNN       |              |     |     |     |
| ResGRU-CNN     |              |     |     |     |
| BiGRU-CNN      |              |     |     |     |
| AttnGRU-CNN    |              |     |     |     |
| AttnLSTM-CNN   |              |     |     |     |
| AttnResGRU-CNN |              |     |     |     |

For visualizations of training and model evaluation, refer to images in the `imgs` directory.

## Contributing

The coding style is dictated by [black](https://black.readthedocs.io/en/stable/). Depending on development environment, you can toggle format-on-save options in your code editor or set up [pre-commit hooks](https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks) to make the linter run on every push.

Please feel free to submit issues or pull requests if you find bugs or ways to optimize the code base. Emails to jaesungtae@gmail.com is also welcome!

## References

[1] Malware Detection by Eating a Whole EXE

```
@misc{raff2017malware,
      title={Malware Detection by Eating a Whole EXE},
      author={Edward Raff and Jon Barker and Jared Sylvester and Robert Brandon and Bryan Catanzaro and Charles Nicholas},
      year={2017},
      eprint={1710.09435},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
```

[2] Learning the PE Header, Malware Detection with Minimal Domain Knowledge

```
@article{Raff_2017,
   title={Learning the PE Header, Malware Detection with Minimal Domain Knowledge},
   ISBN={9781450352024},
   url={http://dx.doi.org/10.1145/3128572.3140442},
   DOI={10.1145/3128572.3140442},
   journal={Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security - AISec  â€™17},
   publisher={ACM Press},
   author={Raff, Edward and Sylvester, Jared and Nicholas, Charles},
   year={2017}
}
```

[3] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

```
@misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

[4] Longformer: The Long-Document Transformer

```
@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer},
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
