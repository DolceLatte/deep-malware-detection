import torch
import torch.nn.functional as F
from torch import nn


class ConvGLU(nn.Conv1d):
    def forward(self, x):
        conv_out = super(ConvGLU, self).forward(x)
        glu_out = F.glu(conv_out, dim=1)
        try:
            return x + glu_out
        except RuntimeError:
            return x + glu_out[:, :, :-1]

class MalConvBase(nn.Module):
    def __init__(self):
        pass

    def forward(self, x):
        pass


class MalConvPlus(nn.Module):
    def __init__(
        self, embed_dim, max_len, kernel_size, n_layers, dropout, device,
    ):
        super(MalConv, self).__init__()
        self.device = device
        self.tok_embed = nn.Embedding(257, embed_dim)
        self.pos_embed = nn.Embedding(max_len, embed_dim)
        self.dropout = nn.Dropout(dropout)
        padding = (kernel_size - 1) // 2
        self.convs = nn.ModuleList(
            [
                ConvGLU(
                    in_channels=embed_dim,
                    out_channels=embed_dim * 2,
                    kernel_size=kernel_size,
                    padding=padding if kernel_size % 2 else padding + 1,
                )
                for _ in range(n_layers)
            ]
        )

        self.fc = nn.Linear(embed_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1)
        # x.shape == (batch_size, byte_len)
        tok_embedding = self.tok_embed(x)
        # tok_embedding.shape == (batch_size, byte_len, embed_dim)
        pos = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(device)
        pos_embedding = self.pos_embed(pos)
        # pos_embedding.shape == (batch_size, byte_len, embed_dim)
        embedding = self.dropout(tok_embedding + pos_embedding)
        conv_in = embedding.permute(0, 2, 1)
        # conv_in.shape = (batch_size, embed_dim, byte_len)

        for layer in self.convs:
            conv_in = layer(conv_in)

        # conv_out = self.conv(conv_in)

        # conv_out.shape == (batch_size, embed_dim, ...)
        values, indices = conv_in.max(dim=-1)
        # values.shape = (batch_size, embed_dim)
        output = self.sigmoid(self.fc(values))
        # output.shape == (batch_size, 1)
        return output


# BATCH_SIZE = 32
# BYTE_LEN = 1000
EMBED_DIM = 32
MAX_LEN = 4097
KERNEL_SIZE = 128
N_LAYERS = 2
DROPOUT = 0.5

test_in = torch.randint(low=0, high=255, size=(32, 1000)).to(device)
model = MalConv(EMBED_DIM, MAX_LEN, KERNEL_SIZE, N_LAYERS, DROPOUT, device).to(device)
test_out = model(test_in)
test_out.shape
