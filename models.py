import torch
import torch.nn.functional as F
from torch import nn


class MalConvBase(nn.Module):
    def __init__(
        self, embed_dim, max_len, out_channels, window_size, dropout,
    ):
        super(MalConvBase, self).__init__()
        self.embed = nn.Embedding(257, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.conv = nn.Conv1d(
            in_channels=embed_dim,
            out_channels=out_channels * 2,
            kernel_size=window_size,
            stride=window_size,
        )
        self.fc = nn.Linear(out_channels, 1)

    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1)
        embedding = self.dropout(self.embed(x))
        conv_in = embedding.permute(0, 2, 1)
        conv_out = self.conv(conv_in)
        glu_out = F.glu(conv_out, dim=1)
        values, indices = glu_out.max(dim=-1)
        output = self.fc(values)
        return output


class MalConvPlus(nn.Module):
    def __init__(
        self, embed_dim, max_len, out_channels, window_size, dropout, device,
    ):
        super(MalConvPlus, self).__init__()
        self.device = device
        self.tok_embed = nn.Embedding(257, embed_dim)
        self.pos_embed = nn.Embedding(max_len, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.conv = nn.Conv1d(
            in_channels=embed_dim,
            out_channels=out_channels * 2,
            kernel_size=window_size,
            stride=window_size,
        )
        self.fc = nn.Linear(out_channels, 1)

    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1)
        # x.shape == (batch_size, byte_len)
        tok_embedding = self.tok_embed(x)
        # tok_embedding.shape == (batch_size, byte_len, embed_dim)
        pos = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).to(device)
        pos_embedding = self.pos_embed(pos)
        # pos_embedding.shape == (batch_size, byte_len, embed_dim)
        embedding = self.dropout(tok_embedding + pos_embedding)
        # embedding.shape == (batch_size, byte_len, embed_dim)
        conv_in = embedding.permute(0, 2, 1)
        # conv_in.shape = (batch_size, embed_dim, byte_len)
        conv_out = self.conv(conv_in)
        glu_out = F.glu(conv_out, dim=1)
        # glu_out.shape == (batch_size, embed_dim, byte_len // window_size)
        values, indices = glu_out.max(dim=-1)
        # values.shape = (batch_size, embed_dim)
        output = self.fc(values)
        # output.shape == (batch_size, 1)
        return output


class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()

    def forward(self, x):
        pass


EMBED_DIM = 32
MAX_LEN = 4096
WINDOW_SIZE = 64
OUT_CHANNELS = 128
DROPOUT = 0.5

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
test_in = torch.randint(low=0, high=255, size=(32, 4096)).to(device)
model_1 = MalConvPlus(
    EMBED_DIM, MAX_LEN, OUT_CHANNELS, WINDOW_SIZE, DROPOUT, device
).to(device)
model_2 = MalConvBase(EMBED_DIM, MAX_LEN, OUT_CHANNELS, WINDOW_SIZE, DROPOUT).to(device)
test_out_1 = model_1(test_in)
test_out_2 = model_2(test_in)
print(test_out_1.shape)
print(test_out_2.shape)
